---
layout: post
#标题配置
title:  Elasticsearch 分词器
#时间配置
date:   2020-03-11 22:30:00 +0800
#大类配置
categories: 数据搜索
#小类配置
tag: ElasticSearch
---

* content
{:toc}

## 说明
无论是内置的分析器（analyzer），还是自定义的分析器（analyzer），都由三种构件块组成的：character filters ， tokenizers ， token filters。

内置的analyzer将这些构建块预先打包到适合不同语言和文本类型的analyzer中。


<b>Character filters （字符过滤器）</b>

字符过滤器以字符流的形式接收原始文本，并可以通过添加、删除或更改字符来转换该流。

举例来说，一个字符过滤器可以用来把阿拉伯数字（٠‎١٢٣٤٥٦٧٨‎٩）‎转成成Arabic-Latin的等价物（0123456789）。

一个分析器可能有0个或多个字符过滤器，它们按顺序应用。

（PS：类似Servlet中的过滤器，或者拦截器，想象一下有一个过滤器链）

<b>Tokenizer （分词器）</b>

一个分词器接收一个字符流，并将其拆分成单个token （通常是单个单词），并输出一个token流。例如，一个whitespace分词器当它看到空白的时候就会将文本拆分成token。它会将文本“Quick brown fox!”转换为[Quick, brown, fox!]


（PS：Tokenizer 负责将文本拆分成单个token ，这里token就指的就是一个一个的单词。就是一段文本被分割成好几部分，相当于Java中的字符串的 split ）

分词器还负责记录每个term的顺序或位置，以及该term所表示的原单词的开始和结束字符偏移量。（PS：文本被分词后的输出是一个term数组）

一个分析器必须只能有一个分词器

<b>Token filters （token过滤器）</b>

token过滤器接收token流，并且可能会添加、删除或更改tokens。

例如，一个lowercase token filter可以将所有的token转成小写。stop token filter可以删除常用的单词，比如 the 。synonym token filter可以将同义词引入token流。

不允许token过滤器更改每个token的位置或字符偏移量。

一个分析器可能有0个或多个token过滤器，它们按顺序应用。

    小结&回顾

        analyzer（分析器）是一个包，这个包由三部分组成，分别是：character filters （字符过滤器）、tokenizer（分词器）、token filters（token过滤器）
        一个analyzer可以有0个或多个character filters
        一个analyzer有且只能有一个tokenizer
        一个analyzer可以有0个或多个token filters
        character filter 是做字符转换的，它接收的是文本字符流，输出也是字符流
        tokenizer 是做分词的，它接收字符流，输出token流（文本拆分后变成一个一个单词，这些单词叫token）
        token filter 是做token过滤的，它接收token流，输出也是token流
        由此可见，整个analyzer要做的事情就是将文本拆分成单个单词，文本 ---->  字符  ---->  token



## 内置的分词器
```bash
# 标准分词(默认)，按词切分，小写处理
GET /_analyze
{
  "analyzer": "standard",
  "text": "Mastering Elasticsearch 3 computor-game this country my name in china"
}

# 简单分词，非字母去除，小写处理
GET /_analyze
{
  "analyzer": "simple",
  "text": "mastering Elasticsearch 3 computor-game this country my name in china"
}

# 空格分词，按空格切分
GET /_analyze
{
  "analyzer": "whitespace",
  "text": "mastering Elasticsearch 3 computor-game this country my name in china"
}

# stop分词，去除修饰词
GET /_analyze
{
  "analyzer": "stop",
  "text": "mastering Elasticsearch 3 computor-game this country my name in china"
}

# Keyword分词，不分词，直接当一个term输出
GET /_analyze
{
  "analyzer": "keyword",
  "text": "mastering Elasticsearch 3 computor-game this country my name in china"
}

```

## 自定义分词器
除了官方提供的分词器，针对特殊的业务查询要求，还可以自定义分词器
```bash
DELETE my_index
# 创建索引时自定义analyzer，char_filter替换特殊表情，根据pattern规则分成单个token，filter过滤token
PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "char_filter": [
            "emotions"
          ],
          "tokenizer": "punctuation",
          "filter": [
            "lowercase",
            "english_stop"
          ]
        }
      },
      "tokenizer": {
        "punctuation": {
          "type": "pattern",
          "pattern": "[ .,!?]"
        }
      },
      "char_filter": {
        "emotions": {
          "type": "mapping",
          "mappings": [
            ":) => _happy_",
            ":( => _sad_"
          ]
        }
      },
      "filter": {
        "english_stop": {
          "type": "stop",
          "stopwords": "_english_"
        }
      }
    }
  }
}

# 测试分词效果
POST my_index/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text": [
    "I'm a :) person, and you?"  
  ]
}
```

## 中文分词器安装及使用
### 下载地址
下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases  
7.5.1版本下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.5.1/elasticsearch-analysis-ik-7.5.1.zip  
### docker方式安装
1.查看es容器id：docker ps  
2.进入docker容器： docker exec -it 容器id /bin/bash  
3.执行安装：./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.5.1/elasticsearch-analysis-ik-7.5.1.zip  
4.退出docker容器：exit  
5.重启容器：docker restart 容器id
### 查看分词效果
